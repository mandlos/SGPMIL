data:
  dataset_config: ../dataset_dependent_configs/tcga-nsclc/nsclc_univ1_dt_config.yaml
  artifact_dir: ../dataset_dependent_configs/tcga-nsclc/out/bayesmil/nsclc_uni_lvl2
  data_dims: 512 # Embeddings dimensions Resnet50:1024, CONCH: 512, UNIv1: 1024, HOPTIMUS: 1536
  num_classes: 2
  use_h5: True
  patch_size: 224
  split: 0 # Split number, must be int

model:
  attention: bayesmil-spvis # [clam, transmil, mil, dgrmil, bayesmil-spvis]
  subtyping: True
  model_size: big # [big, small]
  feature_extractor: univ1_lvl2 # [resnet50, conch, univ1, hoptimus]
  gate: True
  topk: 1 
  dropout: 0.4 # bag classifier dropout rate, doesn't do shit here
  bag_loss_fn: 'ce' # [ce, svm, bcewlogits] for abmil, transmil, clam, does not work for dgrmil
  attn_mode: null
  variant: null # [sb, mb] for single or multi branch
  embed_dim: null
  L: null
  num_les: null # Number of representation for normal/lesion
  dropout_patch: null
  B: null
  instance_loss_fn: null
  bag_weight: null
  print_model: True

logging:
  save_predictions: True
  run_name: test_run  # Can be set to auto
  tracking_url: ./mlruns
  high_conf_metrics: False
  wandb: True
  project: NSCLC  # Name of the project in wandb i.e. Dataset name
  model_ckpt_dir: ../experiments/bayesmil_spvis # Path to the directory where the model checkpoints will be saved
  model_version: v2 # v0: vis, v1: sdpr/enc, v2: apcrf

seed: 2025   # Seed for everything with the seed_torch function from main.py
phase: train # train, test

training:
  max_epochs: 5
  gpu_index: [2]
  strategy: auto
  precision: '32' # 32 or 64 (64 if gpu has enough memory)
  patience: 200 # Early stopping patience
  optimizer: adam # [adam, sgd, lookahead_radam]
  reg: 1.e-5 # weight decay
  opt_eps: 1.e-8 # Adam epsilon not implemented currently
  opt_betas: [0.9, 0.999] # Adam betas, not implemented currently
  momentum: 0.9 # sgd momentum, doesn't do anything for other optimizers
  scheduler: linearcosine # [lambda, cosine, linearcosine]
  learning_rate: 1.e-3  # 0.0001 reasonable choice
  # stuff exclusively for lamda scheduler
  lr_decay_after_epoch: 41    # Tune the decay factors
  stop_decay_after_epoch: 42
  stop_decay_lr_value: 2.e-5 # Stop decaying the learning rate after this lr value
  lr_decay_factor: 0.5
  min_delta: 1.e-4
  # stuff exclusively for cosine/linearcosine scheduler
  min_lr: 1.e-9  # eta_min
  # stuff exclusively for linearcosine scheduler
  warmup_epochs: 1 # 20
  warmup_lr: 1.e-5 # warmup start learning_rate
  lr_logging_interval: epoch  # Log learning rate ['step', 'epoch', None]
  lr_logging_frequency: 1 # Log learning rate every n steps/epochs
  log_weight_decay: True

testing:
  experiment_ckpt_dir: ../experiments/bayesmil_spvis_v2/UNIv1_NSCLC_10x_features/0/last.ckpt

