data:
  dataset_config: ../dataset_dependent_configs/panda/panda_univ1_dt_config.yaml
  artifact_dir: ../dataset_dependent_configs/panda/out/nngp/panda_univ1_lvl1
  data_dims: 1024    # Embeddings dimensions Resnet50:1024, CONCH: 512, UNIv1: 1024, HOPTIMUS: 1536
  use_h5: False
  split: 3

model:
  attention: sgpmil
  attention_multiplication_type: elementwise # [elementwise, relative_error]
  jitter: 1.e-4 # Jitter to add in the diagonal for the kernel
  skip_connection: False # calculate slide representation as Σ(1+a)x or just Σax.
  attn_hl_activation: sigmoid # Activation before gp layer ['sigmoid', 'tanh']
  post_attention_activation: sigmoid # Activation for the output of the SGP layer ['softmax', 'sigmoid', 'tanh']
  gamma: 1 # Weight for max differential entropy regularizer (positive slides)---a good value is 1
  delta: 1 # Weight for the min differential entropy regularizer for negative slides
  mse_factor: 1 # MSE loss weight
  entropy_mode: None # min or max to minimize or maximize the patch attention distribution entropy
  hidden_layer_size_0: 768 # Must be non-zero
  hidden_layer_size_1: 384
  hidden_layer_size_2: 256
  hidden_layer_size_att: 64   # Agp authors propose 64 (Arne Schmidt)
  inducing_points: 80    # Should not become excessively large i.e. > 100
  prior_mean: 0.0      # Prior mean of inducing points distribution p(u), u=f(Z) i.e. a factor that multiplies torch.ones() so if 0.5 then p(u).mean = 0.5
  prior_variance: 1.    # Vanilla implementation uses N(0, I), multply the variance terms by this factor.
  kl_factor: 1   # Factor to multiply regularizer kl_loss i.e. KL(q(u)||p(u)).
  batch_norm_pre_attn: False # Batch normalization before the sgp_layer
  load_model: False # 'None' or path to experiment folder which contains 'models' directory with .h5 files
  attention_activation_temperature: 1. # Temperature for the softmax in the attention mechanism i.e. softmax(f/T) where f are gp function samples for one slide
  classification_softmax_temperature: 1. # Temperature for the softmax in the classification layer i.e. softmax(logits/T)
  save_model: True # True or False
  mc_samples: 30 # Number of MC samples from the prior f|u (direct output of the GP layer)
  kernel_hyperparams: {amp: 0.5, lengthscale_constraint: 0.1, outputscale_constraint: 1.e-2} # Implemented as softplus(y), y=0.1xamp and 10xlengthscale
  kernel: rbf # Type of kernel for the GP layer, 'rbf' or 'matern' or 'matern+rbf'
  print_model: True # Print the model before training starts
  filter_uncertain_patches: False
  q: 0.2 # bottom quantile of uncertain patches to filter
  sampling: var # var, cov, cov2
  # For nnodgp
  N_inducing_points_mean: 128
  N_inducing_points_cov: 128

logging:
  save_predictions: True
  run_name: test_run  # Can be set to auto
  tracking_url: ./mlruns
  high_conf_metrics: False
  wandb: True
  project: PANDA  # Name of the project in wandb i.e. Dataset name
  model_ckpt_dir: ../experiments/nngp # Path to the directory where the model checkpoints will be saved
  model_version: v13 # Version of the model, according to ./custom_framework/heatmaps/results/model_version.json


seed: 2025   # Seed for everything with the seed_torch function from main.py
phase: test # train, test

training:
  max_epochs: 30
  gpu_index: [4]
  strategy: auto
  precision: '32' # 32 or 64 (64 if gpu has enough memory)
  patience: 20 # Early stopping patience
  optimizer: adamw # [adam, adamw, sgd, lookahead_radam]
  reg: 5.e-5 # weight decay
  opt_eps: 1.e-8 # Adam epsilon not implemented currently
  opt_betas: [0.9, 0.999] # Adam betas, not implemented currently
  momentum: 0.9 # sgd momentum, doesn't do anything for other optimizers
  scheduler: cosine # [lambda, cosine, linearcosine]
  learning_rate: 2.e-4  # 0.0001 reasonable choice
  # stuff exclusively for lamda scheduler
  lr_decay_after_epoch: 41    # Tune the decay factors
  stop_decay_after_epoch: 42
  stop_decay_lr_value: 2.e-5 # Stop decaying the learning rate after this lr value
  lr_decay_factor: 0.5
  min_delta: 1.e-4
  # stuff exclusively for cosine/linearcosine scheduler
  min_lr: 1.e-7  # eta_min
  # stuff exclusively for linearcosine scheduler
  warmup_epochs: 1 # ~10-20
  warmup_lr: 1.e-5 # warmup start learning_rate
  lr_logging_interval: epoch  # Log learning rate ['step', 'epoch', None]
  lr_logging_frequency: 1 # Log learning rate every n steps/epochs
  log_weight_decay: True


testing:
  experiment_ckpt_dir: /home/ubuntu/Code/custom_framework/experiments/nngp_v13/UNIv1_PANDA_20x/0/last.ckpt